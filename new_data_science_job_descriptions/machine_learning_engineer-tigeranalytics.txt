Providing solutions for the deployment, execution, validation, monitoring, and improvement of data science solutions Creating Scalable Machine Learning systems that are highly performant Building reusable production data pipelines for implemented machine learning models Writing production-quality code and libraries that can be packaged as containers, installed and deployed Bachelor's degree or higher in computer science or related, with 5+ years of work experience Ability to collaborate with Data Engineers and Data Scientists to build data and model pipelines and help run machine learning tests and experiments Ability to manage the infrastructure and data pipelines needed to bring ML solutions to production End-to-end understanding of applications being created Ability to maintain scalable machine learning solutions in production Ability to abstract the complexity of production for machine learning using containers Ability to troubleshoot production machine learning model issues, including recommendations for to retrain and revalidate Experience with Big Data Projects using multiple types of structured and unstructured data Ability to work with a global team, playing a key role in communicating problem context to the remote teams Excellent communication and teamwork skills Python, Spark, Hadoop, and Docker with an emphasis on good coding practices in a continuous integration context, model evaluation, and experimental design Knowledge of ML frameworks like Scikitlearn, Tensorflow, and Keras Knowledge of MLflow, Airflow, and Kubernetes Experience with Cloud environments and knowledge of offerings such as AWS SageMaker Proficiency in statistical tools, relational databases, and expertise in programming languages (Python/SQL)
